{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, InputLayer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (1089, 7)\n",
      "Date          object\n",
      "Open         float64\n",
      "High         float64\n",
      "Low          float64\n",
      "Close        float64\n",
      "Adj Close    float64\n",
      "Volume       float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>1196.400024</td>\n",
       "      <td>1212.400024</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1201.900024</td>\n",
       "      <td>1201.900024</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>1204.300049</td>\n",
       "      <td>1227.300049</td>\n",
       "      <td>1204.300049</td>\n",
       "      <td>1225.000000</td>\n",
       "      <td>1225.000000</td>\n",
       "      <td>209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>1221.699951</td>\n",
       "      <td>1239.000000</td>\n",
       "      <td>1221.699951</td>\n",
       "      <td>1238.400024</td>\n",
       "      <td>1238.400024</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>1232.800049</td>\n",
       "      <td>1247.000000</td>\n",
       "      <td>1221.900024</td>\n",
       "      <td>1237.800049</td>\n",
       "      <td>1237.800049</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>1239.300049</td>\n",
       "      <td>1242.400024</td>\n",
       "      <td>1226.300049</td>\n",
       "      <td>1229.400024</td>\n",
       "      <td>1229.400024</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2013-12-31  1196.400024  1212.400024  1182.000000  1201.900024   \n",
       "1  2014-01-02  1204.300049  1227.300049  1204.300049  1225.000000   \n",
       "2  2014-01-03  1221.699951  1239.000000  1221.699951  1238.400024   \n",
       "3  2014-01-06  1232.800049  1247.000000  1221.900024  1237.800049   \n",
       "4  2014-01-07  1239.300049  1242.400024  1226.300049  1229.400024   \n",
       "\n",
       "     Adj Close  Volume  \n",
       "0  1201.900024   124.0  \n",
       "1  1225.000000   209.0  \n",
       "2  1238.400024   142.0  \n",
       "3  1237.800049   127.0  \n",
       "4  1229.400024    73.0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "data_csv = \"dataset.csv\"\n",
    "df = pd.read_csv(data_csv)\n",
    "print('Dataset shape: ', df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Open  High  Low  Close  Adj Close  Volume\n",
      "127   NaN   NaN  NaN    NaN        NaN     NaN\n",
      "230   NaN   NaN  NaN    NaN        NaN     NaN\n",
      "248   NaN   NaN  NaN    NaN        NaN     NaN\n",
      "515   NaN   NaN  NaN    NaN        NaN     NaN\n",
      "535   NaN   NaN  NaN    NaN        NaN     NaN\n",
      "Dataset shape:  (1081, 7)\n"
     ]
    }
   ],
   "source": [
    "# Verifying null values and deleting name from dataset\n",
    "null_columns=df.columns[df.isnull().any()]\n",
    "print(df[df.isnull().any(axis=1)][null_columns].head())\n",
    "# Drop the lines with null values\n",
    "df = df.dropna()\n",
    "# Drop Date column\n",
    "# df.pop(\"Date\")\n",
    "\n",
    "print('Dataset shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 1046.199951\n",
      "Maximum: 1391.400024\n",
      "Mean: 1239.990934614246\n",
      "Median: 1251.0\n",
      "SD: 72.09937994027128\n",
      "Skewness: -0.5455305158449637\n",
      "Kurtosis: -0.3520037164214358\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum: {}\\nMaximum: {}\\nMean: {}\\nMedian: {}\\nSD: {}\\nSkewness: {}\\nKurtosis: {}\".format(df[\"Low\"].min(), df[\"High\"].max(), \n",
    "df[\"Open\"].mean(), df[\"Open\"].median(), df[\"Open\"].std(), df[\"Open\"].skew(), df[\"Open\"].kurtosis()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (1007,) Test:  (74,)\n"
     ]
    }
   ],
   "source": [
    "lastday_2017 = df.loc[df[\"Date\"]==\"2017-12-29\"].index.values[0]\n",
    "df = df[\"Close\"].values\n",
    "\n",
    "# Transforming the dataset to ln scale\n",
    "df = np.log(df)\n",
    "\n",
    "# # Split dataset into train and test\n",
    "train_set = df[0:lastday_2017]\n",
    "test_set = df[lastday_2017:]\n",
    "print(\"Train: \", train_set.shape, \"Test: \", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastday_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df   = df.reshape(len(df),1)\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler.fit(df)\n",
    "# df = scaler.transform(df).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels of an LSTM network\n",
    "Now for the labels of the LSTM network, not all the cases require a label matrix of 3 dimensions. The cases where this is required are on sequence to sequence problems, where the model is made to predict a sequence of timestamps of one or more features. However for our problem, the LSTM network needs to predict the next day gold price closing value, this way a matrix of 2 dimensions will suffice for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN class\n",
    "class FFNN:\n",
    "    def __init__(self, input_dim, scaler=None):\n",
    "        self.scaler = scaler \n",
    "        optimizer = Adam()\n",
    "        h_n = 3 if input_dim == 4 or input_dim == 6 else 5\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(h_n, input_dim=input_dim))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"accuracy\", \"mean_absolute_error\"])\n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        if self.scaler:\n",
    "            x_train = self.scaler.transform(x_train)\n",
    "            y_train = self.scaler.transform(y_train)\n",
    "        self.model.fit(x_train, y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=128,\n",
    "                        verbose=0\n",
    "                      )\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        if self.scaler:\n",
    "            x_test = self.scaler.transform(x_test)\n",
    "            \n",
    "        y_valid_pred = self.model.predict(x_test)\n",
    "        \n",
    "        if self.scaler:\n",
    "            y_valid_pred = self.scaler.transform(y_valid_pred)\n",
    "\n",
    "        return y_valid_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM class\n",
    "class PLSTM:\n",
    "    def __init__(self, input_shape, model_type, scaler=None):\n",
    "        self.scaler = scaler \n",
    "        optimizer = Adam()\n",
    "        self.model = Sequential()\n",
    "        self.h_n1 = 100 if model_type in [1,3,4] else 200\n",
    "        return_seq = True if model_type>2 else False\n",
    "        self.model.add(LSTM(units=self.h_n1, input_shape=(input_shape[1], 1), return_sequences=return_seq))\n",
    "        if model_type>2:\n",
    "            self.h_n2 = 50 if model_type == 3 else 100\n",
    "            self.model.add(LSTM(units=self.h_n2))\n",
    "            if type==4:\n",
    "                self.model.add(Dense(32))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"accuracy\", \"mean_absolute_error\"])\n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        if self.scaler:\n",
    "            x_train = self.scaler.transform(x_train)\n",
    "            y_train = self.scaler.transform(y_train)\n",
    "\n",
    "        # reshape the entry as a 3D matrix with samples, timestamps and lastly features\n",
    "        # instead of only samples and features as usual.\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "        history = self.model.fit(x_train, y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=128,\n",
    "                        verbose=0)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        if self.scaler:\n",
    "            x_test = self.scaler.transform(x_test)\n",
    "            \n",
    "        x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "        y_valid_pred = self.model.predict(x_test)\n",
    "        \n",
    "        if self.scaler:\n",
    "            y_valid_pred = self.scaler.transform(y_valid_pred)\n",
    "\n",
    "        return y_valid_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-LSTM class\n",
    "class CNNLSTM:\n",
    "    def __init__(self, input_shape, model_type, scaler=None):\n",
    "        self.scaler = scaler\n",
    "        optimizer = Adam()\n",
    "        self.model = Sequential()\n",
    "        self.h_n1 = 100 if model_type == 1 else 200\n",
    "        self.filter1 = 32 if model_type == 1 else 64\n",
    "        self.filter2 = 64 if model_type == 1 else 128\n",
    "        \n",
    "        self.model.add(Conv1D(self.filter1, 2,activation='relu',\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       input_shape=(input_shape[1],\n",
    "                                   1)))\n",
    "\n",
    "        self.model.add(Conv1D(self.filter2, 2,\n",
    "                   activation='relu',\n",
    "                   strides=1,\n",
    "                   padding='same',\n",
    "                   input_shape=(input_shape[1],\n",
    "                                1)))\n",
    "\n",
    "        self.model.add(MaxPooling1D(pool_size=2, padding='valid'))\n",
    "        self.model.add(LSTM(units=self.h_n1, input_shape=(input_shape[1],1)))\n",
    "    \n",
    "        if type==2:\n",
    "            self.model.add(Dense(32))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        if self.scaler:\n",
    "            x_train = self.scaler.transform(x_train)\n",
    "            y_train = self.scaler.transform(y_train)\n",
    "            \n",
    "        # reshape the entry as a 3D matrix with samples, timestamps and lastly features\n",
    "        # instead of only samples and features as usual.\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "        \n",
    "        self.model.fit(x_train, y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=128,\n",
    "                        verbose=0\n",
    "                      )\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        if self.scaler:\n",
    "            x_test = self.scaler.transform(x_test)\n",
    "            \n",
    "        x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "        y_valid_pred = self.model.predict(x_test)\n",
    "        \n",
    "        if self.scaler:\n",
    "            y_valid_pred = self.scaler.transform(y_valid_pred)\n",
    "\n",
    "        return y_valid_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(entry_shape, scaler=None):\n",
    "    # SVR\n",
    "    svr = SVR(kernel='rbf', C=1, tol=1e-3)\n",
    "\n",
    "    # FFNN\n",
    "    ffnn = FFNN(entry_shape[1], scaler)\n",
    "\n",
    "    # LSTM1\n",
    "    lstm1 = PLSTM(entry_shape, 1, scaler)\n",
    "\n",
    "    # LSTM2\n",
    "    lstm2 = PLSTM(entry_shape, 2, scaler)\n",
    "\n",
    "    # LSTM3\n",
    "    lstm3 = PLSTM(entry_shape, 3, scaler)\n",
    "\n",
    "    # LSTM4\n",
    "    lstm4 = PLSTM(entry_shape, 4, scaler)\n",
    "\n",
    "    # CNN-LSTM1\n",
    "    cnnlstm1 = CNNLSTM(entry_shape, 1, scaler)\n",
    "\n",
    "    # CNN-LSTM2\n",
    "    cnnlstm2 = CNNLSTM(entry_shape, 2, scaler)\n",
    "\n",
    "    labels = [\"SVR\", \"FFNN\", \"LSTM1\", \"LSTM2\", \"LSTM3\", \"LSTM4\", \"CNN-LSTM1\", \"CNN-LSTM2\"]\n",
    "    models = [svr, ffnn, lstm1, lstm2, lstm3, lstm4, cnnlstm1, cnnlstm2]\n",
    "\n",
    "    return labels, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling window approach\n",
    "The paper states that in order to predict the next day gold price, the model uses the $n$ past days gold prince, where $n$ stands for the time horizon used. Thus to generate a dataset with this specifications, we will use a rolling window algorithm to generate a window of features to a window of labels ( with in this case is equal to 1). This rolling window procedure works as follows:\n",
    "\n",
    "Features: $[n1, n2, n3, n4, n5]$ -> Label $[n6]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_mtx(x, window_size):\n",
    "        \"\"\"Compute all overlapping (rolling) observation windows over a vector \n",
    "            and return a matrix\n",
    "\n",
    "        Args:\n",
    "            x           : observation vector that is supposed to be split into\n",
    "                          overlapping windows\n",
    "            window_size : the target window size\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Window matrix with all windows as rows. That is, if n_windows is the\n",
    "            number of windows, the result has dimensions:\n",
    "\n",
    "            (n_windows, window_size)\n",
    "\n",
    "        \"\"\"\n",
    "        if window_size < 1:\n",
    "            raise ValueError(\"`window_size` must be at least 1.\")\n",
    "        if window_size > x.shape[-1]:\n",
    "            raise ValueError(\"`window_size` is too long.\")\n",
    "\n",
    "        shape = x.shape[:-1] + (x.shape[-1] - window_size + 1, window_size)\n",
    "        strides = x.strides + (x.strides[-1],)\n",
    "\n",
    "        return np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feat_labels_per_horizon(time_horizon, df, verbose=False):\n",
    "\n",
    "    # Get the feature and label to the prediction task \n",
    "    feature_mtx = rolling_window_mtx(df, time_horizon)[:-1]\n",
    "    label_mtx   = rolling_window_mtx(df[time_horizon:], 1)\n",
    "    index_mtx   = rolling_window_mtx(np.arange(len(df)), time_horizon)\n",
    "\n",
    "    if verbose:\n",
    "        # Now we have a set of windows of the real coordinate\n",
    "        # Lets take a look in one window\n",
    "        print(f\"\\n One feature window: \\n {feature_mtx[0]}\")\n",
    "        print(f\"\\n One label window: \\n {label_mtx[0]}\")\n",
    "        print(f\"\\n Original dataset: \\n {df[0:5]}\")\n",
    "\n",
    "    # For the classification task (if the gold values goes up or down)\n",
    "    # We need to get a window of size 2, and then calculate the difference\n",
    "    # If positive, the gold value went up.\n",
    "    class_label_mtx = rolling_window_mtx(df[time_horizon-1:], 2)\n",
    "    func = lambda x: True if x > 0 else False\n",
    "    class_func = np.vectorize(func)\n",
    "    class_label_mtx = class_func(np.diff(class_label_mtx).flatten()).reshape(len(class_label_mtx),1)\n",
    "   \n",
    "    if verbose:\n",
    "    \n",
    "        print(f\"\\n One window of class label (If tomorrow price is larger than today's price): \\n {class_label_mtx[0]}\")\n",
    "    \n",
    "    return feature_mtx, label_mtx, class_label_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index_mtx = rolling_window_mtx(np.arange(len(df))[4:], 1)\n",
    "index_mtx   = rolling_window_mtx(np.arange(len(df)), 4)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1003, 1004, 1005, 1006]), array([1007]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = lastday_2017 - 4\n",
    "index_mtx[train_idx], label_index_mtx[train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.09165894, 7.11069612, 7.12157552, ..., 7.18629566, 7.18258009,\n",
       "       7.1856143 ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reshape(len(df),1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(df)\n",
    "df = df.flatten()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR\n",
      "FFNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaneto/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM1\n",
      "LSTM2\n",
      "LSTM3\n",
      "LSTM4\n",
      "CNN-LSTM1\n",
      "CNN-LSTM2\n",
      "SVR\n",
      "FFNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaneto/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM1\n",
      "LSTM2\n",
      "LSTM3\n",
      "LSTM4\n",
      "CNN-LSTM1\n",
      "CNN-LSTM2\n",
      "SVR\n",
      "FFNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaneto/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM1\n",
      "LSTM2\n",
      "LSTM3\n",
      "LSTM4\n",
      "CNN-LSTM1\n",
      "CNN-LSTM2\n"
     ]
    }
   ],
   "source": [
    "entries = [4, 6, 9]\n",
    "models = []\n",
    "labels = []\n",
    "for entry in entries:\n",
    "    #Creating dataset\n",
    "    feature_mtx, label_mtx, class_label_mtx = generate_feat_labels_per_horizon(entry, df)\n",
    "    scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_input.fit(feature_mtx)\n",
    "    scaler_output = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_output.fit(label_mtx)\n",
    "    train_idx = lastday_2017 - entry\n",
    "    train_x = feature_mtx[:train_idx]\n",
    "    train_y = label_mtx[:train_idx]\n",
    "    tmp_labels, tmp_models = create_models(train_x.shape)#, scaler)\n",
    "    for i in range(len(tmp_models)):\n",
    "        print(tmp_labels[i])\n",
    "        tmp_models[i].fit(train_x, train_y)\n",
    "    models.append(tmp_models)\n",
    "    labels.append(tmp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "models = np.ravel(models)\n",
    "labels = np.ravel(labels)\n",
    "print(len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_pred(y):\n",
    "    preds = []\n",
    "    for i in range(1, len(y)):\n",
    "        last_y = y[i - 1]\n",
    "        curr_y = y[i]\n",
    "        preds.append(curr_y - last_y > 0.0 )\n",
    "    return np.array(preds)\n",
    "\n",
    "# Metric functions\n",
    "def get_metrics(y, pred_y):\n",
    "    y_classification = classification_pred(y)\n",
    "    y_pred_classification = classification_pred(pred_y)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_classification, y_pred_classification)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(y_classification)):\n",
    "        is_y_pred_up = y_pred_classification[i]\n",
    "        is_y_up = y_classification[i][0]\n",
    "\n",
    "        if is_y_pred_up and is_y_up:\n",
    "            tp += 1\n",
    "        elif is_y_pred_up and not is_y_up:\n",
    "            fp += 1\n",
    "        elif not is_y_pred_up and not is_y_up:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    return tp, tn, fp, fn, auc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Entries:  4\n",
      "\n",
      "Classifier type:  SVR\n",
      "MAE =  0.07038084116745039\n",
      "RMSE =  0.005027129598584784\n",
      "ACC =  0.4657534246575342\n",
      "AUC =  0.4665413533834587\n",
      "SEN =  0.4473684210526316\n",
      "SPE =  0.4857142857142857\n",
      "\n",
      "Classifier type:  FFNN\n",
      "MAE =  0.5049660331265111\n",
      "RMSE =  0.2550771400809821\n",
      "ACC =  0.547945205479452\n",
      "AUC =  0.5477443609022558\n",
      "SEN =  0.5526315789473685\n",
      "SPE =  0.5428571428571428\n",
      "\n",
      "Classifier type:  LSTM1\n",
      "MAE =  0.06302946932539005\n",
      "RMSE =  0.004062881298572275\n",
      "ACC =  0.410958904109589\n",
      "AUC =  0.4116541353383459\n",
      "SEN =  0.39473684210526316\n",
      "SPE =  0.42857142857142855\n",
      "\n",
      "Classifier type:  LSTM2\n",
      "MAE =  0.0597722977976306\n",
      "RMSE =  0.003664960416941944\n",
      "ACC =  0.3972602739726027\n",
      "AUC =  0.3984962406015038\n",
      "SEN =  0.3684210526315789\n",
      "SPE =  0.42857142857142855\n",
      "WARNING:tensorflow:5 out of the last 1465 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f266d63a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM3\n",
      "MAE =  0.07115574106297848\n",
      "RMSE =  0.005156573331385825\n",
      "ACC =  0.4246575342465753\n",
      "AUC =  0.42593984962406023\n",
      "SEN =  0.39473684210526316\n",
      "SPE =  0.45714285714285713\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eab6d4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM4\n",
      "MAE =  0.07093208510454506\n",
      "RMSE =  0.005122704726024091\n",
      "ACC =  0.4246575342465753\n",
      "AUC =  0.42593984962406023\n",
      "SEN =  0.39473684210526316\n",
      "SPE =  0.45714285714285713\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ea8cb8820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  CNN-LSTM1\n",
      "MAE =  0.059554396103822536\n",
      "RMSE =  0.003623668581610973\n",
      "ACC =  0.5068493150684932\n",
      "AUC =  0.5071428571428571\n",
      "SEN =  0.5\n",
      "SPE =  0.5142857142857142\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8edce3d8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  CNN-LSTM2\n",
      "MAE =  0.05244874919715024\n",
      "RMSE =  0.0028220307023227986\n",
      "ACC =  0.5205479452054794\n",
      "AUC =  0.5214285714285715\n",
      "SEN =  0.5\n",
      "SPE =  0.5428571428571428\n",
      "# Entries:  6\n",
      "\n",
      "Classifier type:  SVR\n",
      "MAE =  0.06938287345347131\n",
      "RMSE =  0.00489374749799132\n",
      "ACC =  0.4520547945205479\n",
      "AUC =  0.45000000000000007\n",
      "SEN =  0.5\n",
      "SPE =  0.4\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f3c1c3d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  FFNN\n",
      "MAE =  0.010147363995628728\n",
      "RMSE =  0.00016544871202396052\n",
      "ACC =  0.5342465753424658\n",
      "AUC =  0.5334586466165414\n",
      "SEN =  0.5526315789473685\n",
      "SPE =  0.5142857142857142\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f3404a430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM1\n",
      "MAE =  0.07523400684592756\n",
      "RMSE =  0.005755643143380616\n",
      "ACC =  0.547945205479452\n",
      "AUC =  0.5488721804511277\n",
      "SEN =  0.5263157894736842\n",
      "SPE =  0.5714285714285714\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f3c1c3940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM2\n",
      "MAE =  0.07148132418585049\n",
      "RMSE =  0.005204833336086559\n",
      "ACC =  0.5616438356164384\n",
      "AUC =  0.5609022556390979\n",
      "SEN =  0.5789473684210527\n",
      "SPE =  0.5428571428571428\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ede7d44c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM3\n",
      "MAE =  0.07627469105621512\n",
      "RMSE =  0.005912323897192773\n",
      "ACC =  0.4657534246575342\n",
      "AUC =  0.46541353383458645\n",
      "SEN =  0.47368421052631576\n",
      "SPE =  0.45714285714285713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ec0106ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM4\n",
      "MAE =  0.0768637782589574\n",
      "RMSE =  0.006002607067580226\n",
      "ACC =  0.5068493150684932\n",
      "AUC =  0.5060150375939849\n",
      "SEN =  0.5263157894736842\n",
      "SPE =  0.4857142857142857\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ec33d0940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  CNN-LSTM1\n",
      "MAE =  0.06899522411788701\n",
      "RMSE =  0.004846492827545233\n",
      "ACC =  0.4794520547945205\n",
      "AUC =  0.47857142857142854\n",
      "SEN =  0.5\n",
      "SPE =  0.45714285714285713\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ec08eb430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  CNN-LSTM2\n",
      "MAE =  0.060984050560296224\n",
      "RMSE =  0.003800078703055236\n",
      "ACC =  0.4931506849315068\n",
      "AUC =  0.49398496240601497\n",
      "SEN =  0.47368421052631576\n",
      "SPE =  0.5142857142857142\n",
      "# Entries:  9\n",
      "\n",
      "Classifier type:  SVR\n",
      "MAE =  0.06865705233627735\n",
      "RMSE =  0.004796712147221855\n",
      "ACC =  0.4931506849315068\n",
      "AUC =  0.4928571428571429\n",
      "SEN =  0.5\n",
      "SPE =  0.4857142857142857\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8edd9d6c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  FFNN\n",
      "MAE =  0.011671155103286675\n",
      "RMSE =  0.0001833548731894715\n",
      "ACC =  0.5205479452054794\n",
      "AUC =  0.5203007518796992\n",
      "SEN =  0.5263157894736842\n",
      "SPE =  0.5142857142857142\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8edf22ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM1\n",
      "MAE =  0.08181374747331947\n",
      "RMSE =  0.00679293819230594\n",
      "ACC =  0.547945205479452\n",
      "AUC =  0.5488721804511277\n",
      "SEN =  0.5263157894736842\n",
      "SPE =  0.5714285714285714\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8edd9d6dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM2\n",
      "MAE =  0.07593885593444173\n",
      "RMSE =  0.005861502873364824\n",
      "ACC =  0.4383561643835616\n",
      "AUC =  0.4357142857142857\n",
      "SEN =  0.5\n",
      "SPE =  0.37142857142857144\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8edf22c1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM3\n",
      "MAE =  0.07643749872701458\n",
      "RMSE =  0.005938398007818348\n",
      "ACC =  0.5068493150684932\n",
      "AUC =  0.5037593984962406\n",
      "SEN =  0.5789473684210527\n",
      "SPE =  0.42857142857142855\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eddbc8790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  LSTM4\n",
      "MAE =  0.0764519649353952\n",
      "RMSE =  0.005939794098096753\n",
      "ACC =  0.4657534246575342\n",
      "AUC =  0.46541353383458645\n",
      "SEN =  0.47368421052631576\n",
      "SPE =  0.45714285714285713\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8edf1e8ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier type:  CNN-LSTM1\n",
      "MAE =  0.06755230843341899\n",
      "RMSE =  0.004649191584573999\n",
      "ACC =  0.4931506849315068\n",
      "AUC =  0.4928571428571429\n",
      "SEN =  0.5\n",
      "SPE =  0.4857142857142857\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8f343584c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Classifier type:  CNN-LSTM2\n",
      "MAE =  0.06884851292304961\n",
      "RMSE =  0.004828193286182729\n",
      "ACC =  0.4657534246575342\n",
      "AUC =  0.46541353383458645\n",
      "SEN =  0.47368421052631576\n",
      "SPE =  0.45714285714285713\n"
     ]
    }
   ],
   "source": [
    "# Testing models\n",
    "index_model = 0\n",
    "for entry in entries:\n",
    "    print (\"# Entries: \", entry)\n",
    "    feature_mtx, label_mtx, class_label_mtx = generate_feat_labels_per_horizon(entry, df)\n",
    "    scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_input.fit(feature_mtx)\n",
    "    train_idx = lastday_2017 - entry\n",
    "    test_x = feature_mtx[train_idx:]\n",
    "    test_y = label_mtx[train_idx:]\n",
    "    for i in range(index_model, index_model+8):\n",
    "        test_y_estimative = models[i].predict(test_x)\n",
    "        tp, tn, fp, fn, auc_value = get_metrics(test_y, test_y_estimative)\n",
    "        \n",
    "        print(\"\\nClassifier type: \", labels[i])\n",
    "        print(\"MAE = \", mean_absolute_error(test_y, test_y_estimative))\n",
    "        print(\"RMSE = \", mean_squared_error(test_y, test_y_estimative, squared=True))\n",
    "        print(\"ACC = \", (tp + tn) / (tp + tn + fp + fn))\n",
    "        print(\"AUC = \", auc_value)\n",
    "        print(\"SEN = \", tp / (tp + fn))\n",
    "        print(\"SPE = \", tn / (tn + fp))\n",
    "        \n",
    "    index_model += 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f87398b2de8e90719949ed00c7b022e62357d5ca27b768fb93eb54116fa5b864"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
